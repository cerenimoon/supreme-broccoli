{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ac0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceren umay ozten -181180060 - assignment - 1\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef32458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(sum):\n",
    "    return 1/(1 + np.exp(-sum)) #sigmoid formula for more flexibility\n",
    "\n",
    "def sigmoid_derivative(sigmoid): #sigmoid derivative for backpropagation adjusting weights\n",
    "    return sigmoid*(1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df4fa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20137489, -0.988298  , -0.8350174 ],\n",
       "       [-0.04527924,  0.9616224 , -0.96123865],\n",
       "       [ 0.98564658,  0.85816569,  0.71886479],\n",
       "       [ 0.51725858,  0.45792614, -0.70644273]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W0 = 2*np.random.random((4,3)) - 1 #random W0 values and multiply it by 2 and substract 1 for negative and positive rows\n",
    "W0 #weights for four columns of iris data set with 3 numerals for hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55c70e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36905519],\n",
       "       [-0.44896844],\n",
       "       [-0.94003666]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = 2*np.random.random((3, 1)) - 1 #random W1 values and multiply it by 2 and substract 1 for negative and positive rows\n",
    "W1 #three neuron one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1132d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets #import datasets\n",
    "iris = datasets.load_iris() #load iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e926f9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data #show iris data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f525d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names #columns of iris data sets -inputs-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9cfbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target #output target 0 -> setosa 1 -> versicolor 2 -> virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c349be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have only one neuron output layer because of this reason we use only 0 and 1 targets \n",
    "inputs = iris.data[0:100] #we grap first 100 instances including 0 and 1 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53e9f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs) #shows length of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4e3c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape #shows dimensions of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f844620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = iris.target[0:100] #we create target outputs\n",
    "outputs #just 0 and 1 instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03d8935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs) #length of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524d6534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape #dimensions of output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f620208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = outputs.reshape(-1, 1) #we reshape the outputs from vector type to matrix type same as outputs\n",
    "outputs.shape #100 rows 1 column matrix shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beae93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000 #epoch number 10000 and 3500\n",
    "learning_rate = 0.01 #learning_rate in homework example we choose learning_rate for small value 0.001 then relatively big value 0.3 and lastly 0.01\n",
    "error = [] #error array that takes average error values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0485c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Error: 0.5135677909741987\n",
      "Epoch: 1001 Error: 0.05726801970319144\n",
      "Epoch: 2001 Error: 0.038618557442763636\n",
      "Epoch: 3001 Error: 0.03091517037912328\n",
      "Epoch: 4001 Error: 0.026470684630094287\n",
      "Epoch: 5001 Error: 0.02349745581821209\n",
      "Epoch: 6001 Error: 0.02133267196549617\n",
      "Epoch: 7001 Error: 0.01966727843214793\n",
      "Epoch: 8001 Error: 0.018335469660698402\n",
      "Epoch: 9001 Error: 0.01723936247409856\n"
     ]
    }
   ],
   "source": [
    "#for epoch in epochs:\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    input_layer = inputs #take input values\n",
    "    sum_syn0 = np.dot(input_layer, W0) #multiply input values with weights and sum\n",
    "    hidden_layer = sigmoid(sum_syn0) #activate the summation\n",
    "    \n",
    "    sum_syn1 = np.dot(hidden_layer, W1) #multiply hidden layer activations with weights and sum\n",
    "    output_layer = sigmoid(sum_syn1) #activate the summation\n",
    "    \n",
    "    error_output_layer = (outputs - output_layer) #average error between outputs -predictions- and real outputs\n",
    "    average = np.mean(abs(error_output_layer)) #take the average of error\n",
    "    if epoch%1000 == 0: #shows the epoch according to mod value and epoch number\n",
    "        print('Epoch: ' + str(epoch + 1) + ' Error: ' + str(average)) #print the average error\n",
    "        error.append(average) #add to the error list\n",
    "    \n",
    "    \n",
    "    derivative_output = sigmoid_derivative(output_layer) #derivative of output layer for delta_output\n",
    "    delta_output = error_output_layer*derivative_output #delta_output for gradient \n",
    "    \n",
    "    W1T = W1.T #transpoze of W1 \n",
    "    delta_output_weight = delta_output.dot(W1T) #multiply weights from hidden layer to output layer with delta_output\n",
    "    delta_hidden_layer = delta_output_weight*sigmoid_derivative(hidden_layer) #delta of hidden layer for gradient operation\n",
    "    \n",
    "    hidden_layerT = hidden_layer.T #transpoze of hidden_layer matris\n",
    "    input_x_delta1 = hidden_layerT.dot(delta_output) #multiply hiddenlayer variables with delta_output (gradient)\n",
    "    W1 = W1 + (input_x_delta1*learning_rate) #adjust weights with gradient for hidden layer to output layer\n",
    "    \n",
    "    input_layerT = input_layer.T #transpoze of hidden_layer matris\n",
    "    input_x_delta0 = input_layerT.dot(delta_hidden_layer) #multiply inputlayer variables with delta_hidden_layer (gradient)\n",
    "    W0 = W0 + (input_x_delta0*learning_rate) #adjust weigthts for from hidden_layer to input layer\n",
    "    #we apply backpropagation to adjust weights and calculate derivatives of neuron layer outputs \n",
    "    #when we apply 10000 epochs error decreasement is huge from 0.496462 to 0.0605625 learning_rate = 0.001 and accuracy \n",
    "    #rate is 0.9394374279\n",
    "    #0.001 leerning rate is default learning rate for must applications although low learning rate is slow to converge with\n",
    "    #gradient descent process, low learning rate prevents long distances from global minimum and stuck in local minimum\n",
    "    #This example works well with 10000 epoch value and low learning rate 0.001 \n",
    "    #high learning rate convergence is fast but lose global minimum target\n",
    "    #low learning rate will be slower but more likely to reach the global minimum\n",
    "    #but when we apply 0.3 instead of 0.001 learning rate and 10000 epoch error decreasement is nearly not exist \n",
    "    #from 0.5051860339828742 to 0.4999998406305917 error rate \n",
    "    #Therefore we choose 0.01 learning rate between 0.001 and 0.3 epoch 10000 an optimum value and get huge decreasement from 0.511908 to\n",
    "    #0.010766966 0.98923303 accuracy rate\n",
    "    #On 0.01 learning rate and 10000 epoch we get 0.01076696696658 error rate and 0.01727066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ecc77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892330330334141"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-0.010766966966585823) #accuracy rate of our neural network great accuracy rate on 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "984b0558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28bf59b4348>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Epochs') #plot epoch x - axis line\n",
    "plt.ylabel('Error') #plot error y - axis line\n",
    "plt.plot(error) #plot error array\n",
    "#we see that error graphic follows a smooth path from 0.5 accuracy rate to below 0.0 accuracy rate in 0.01 \n",
    "#but epochs are too much for 0.01 because of this reason we decrease epochs from 10000 to approximately 3500\n",
    "#but this decrasement causing much lower accuracy performance and no change in graphic \n",
    "#therefore we remain in 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f5b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_calculate_pred(instance):\n",
    "    hidden_layer = sigmoid(np.dot(instance, W0)) #create hidden layer with new W0 values\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, W1)) #create output layer with new W1 values\n",
    "    return output_layer[0] #return output value as plain number version for rounding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90bc202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.1, 3.5, 1.4, 0.2]), array([0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0], outputs[0] #shows first sample (iris) input and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66f6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_value = softmax_calculate_pred(inputs[0]) #calculate output for input values since we have one neuron output layer we can predict\n",
    "                                  #0 and 1 target values only !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c971a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_value = round(pred_value) #round the value to get actual target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a89200f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names[round_value] #we get setosa target name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e52e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_method(instance): #we create predict method for easiness \n",
    "    pred_value = softmax_calculate_pred(instance)\n",
    "    round_value = round(pred_value)\n",
    "    return iris.target_names[round_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2fc1d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_method(inputs[99]) #find versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44b4beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our neural network has 4 input neurons, 3 hidden_layer neurons and one output neuron\n",
    "#Our neural network performs well on 0.001 and 0.01 learning_rate values bu not on 0.3 value \n",
    "#Our neural network we use sigmoid activation function and most basic error rate calculation that output - real output\n",
    "#backpropagation process uses gradients with delta outputs with derivatives and error multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786502a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
